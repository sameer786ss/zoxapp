[package]
name = "zox"
version = "0.1.0"
description = "ZOX - AI-Powered Coding Agent"
authors = ["you"]
edition = "2021"

[lib]
name = "zox_lib"
crate-type = ["staticlib", "cdylib", "rlib"]

[build-dependencies]
tauri-build = { version = "2", features = [] }

[dependencies]
tauri = { version = "2", features = [] }
tauri-plugin-opener = "2"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
reqwest = { version = "0.13.1", features = ["json", "stream"] }
tokio = { version = "1.49.0", features = ["full"] }
futures = "0.3.31"
tauri-plugin-fs = "2.4.5"
lancedb = "0.21.0"
fastembed = "5.8.1"
chrono = { version = "0.4.39", features = ["serde"] }
uuid = { version = "1.19.0", features = ["v4", "serde"] }
dashmap = "6.1.0"
parking_lot = "0.12.5"
ignore = "0.4.22"
directories = "5.0.1"
notify = "6.1.1"
portable-pty = "0.8.1"
tree-sitter = "0.24.4"
tree-sitter-rust = "0.23.2"
tree-sitter-typescript = "0.23.2"
window-vibrancy = "0.5"
async-trait = "0.1"
dirs = "5.0"
zip = "0.6"
once_cell = "1.19"
tauri-plugin-updater = "2"

# Candle ML Framework for local LLM inference
candle-core = "0.8"
candle-nn = "0.8"
candle-transformers = "0.8"
tokenizers = { version = "0.20", default-features = false, features = ["onig", "progressbar"] }
hf-hub = "0.3"
byteorder = "1.5"
memmap2 = "0.9"
half = "2.4"
rand = "0.8"

[features]
default = []
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
metal = ["candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]
